
# History of NLP

![image](https://user-images.githubusercontent.com/11299574/129455345-9f2a167b-1446-4895-9c09-4a073fb8f591.png)

[Image Source](https://www.xenonstack.com/blog/evolution-of-nlp) 


Here, is are important events in the history of Natural Language Processing:

1950- NLP started when Alan Turing published an article called "Machine and Intelligence."

1950- Attempts to automate translation between Russian and English

1960- The work of Chomsky and others on formal language theory and generative syntax

1990- Probabilistic and data-driven models had become quite standard

2000- A Large amount of spoken and textual data become available


## History of Natural Language Processing (NLP)

The idea had emerged from the need for Machine Translation in the 1940s.Then the original language was English and Russian.

But the use of other words such as Chinese also came into existence in the initial period of the 1960s.

Then a lousy era came for MT/NLP during 1966, this fact was supported by a report of ALPAC, according to which MT/NLP almost died because the research in this area did not have the pace at that time. 

This condition became better again in the 1980s when the product related to MT/NLP started providing some results to customers. 

After reaching in dying state in the 1960s, the NLP/MT got a new life when the idea and need of Artificial Intelligence emerged. 

LUNAR is developed in 1978 by W.A woods; it could analyze, compare and evaluate the chemical data on a lunar rock and soil composition that was accumulating as a result of Apollo moon missions and can answer the related question. 

In the 1980s the area of computational grammar became a very active field of research which was linked with the science of reasoning for meaning and considering the user â€˜s beliefs and intentions. 

In the period of 1990s, the pace of growth of NLP/MT increased. Grammars, tools and Practical resources related to NLP/MT became available with the parsers. 

The research on the core and futuristic topics such as word sense disambiguation and statistically colored NLP, the work on the lexicon got a direction of research. 

This quest of the emergence of NLP was joined by other essential topics such as statistical language processing, Information Extraction and automatic summarising. 

The discussion on the history of NLP cannot be considered complete without the mention of the ELIZA, a chatbot program which was developed from 1964 to 1966 at the Artificial Intelligence Laboratory of MIT. 

It was created by Joseph Weizenbaum. It was a program which was based on script named as DOCTOR which was arranged to Rogerian Psychotherapist and used rules, to response the questions of the users which were psychometric-based. It was one of the chatbots which were capable of taking the Turing test at that time.

(1940-1960) - Focused on Machine Translation (MT)

The Natural Languages Processing started in the year 1940s.


1948 - In the Year 1948, the first recognisable NLP application was introduced in Birkbeck College, London.

1950s - In the Year 1950s, there was a conflicting view between linguistics and computer science. Now, Chomsky developed his first book syntactic structures and claimed that language is generative in nature.

In 1957, Chomsky also introduced the idea of Generative Grammar, which is rule based descriptions of syntactic structures.

(1960-1980) - Flavored with Artificial Intelligence (AI)

In the year 1960 to 1980, the key developments were:

Augmented Transition Networks (ATN)

Augmented Transition Networks is a finite state machine that is capable of recognizing regular languages.

Case Grammar

Case Grammar was developed by Linguist Charles J. Fillmore in the year 1968. Case Grammar uses languages such as English to express the relationship between nouns and verbs by using the preposition.

In Case Grammar, case roles can be defined to link certain kinds of verbs and objects.

For example: "Neha broke the mirror with the hammer". In this example case grammar identify Neha as an agent, mirror as a theme, and hammer as an instrument.

In the year 1960 to 1980, key systems were:

SHRDLU


SHRDLU is a program written by Terry Winograd in 1968-70. It helps users to communicate with the computer and moving objects. It can handle instructions such as "pick up the green boll" and also answer the questions like "What is inside the black box." The main importance of SHRDLU is that it shows those syntax, semantics, and reasoning about the world that can be combined to produce a system that understands a natural language.

LUNAR

LUNAR is the classic example of a Natural Language database interface system that is used ATNs and Woods' Procedural Semantics. It was capable of translating elaborate natural language expressions into database queries and handle 78% of requests without errors.

1980 - Current

Till the year 1980, natural language processing systems were based on complex sets of hand-written rules. After 1980, NLP introduced machine learning algorithms for language processing.

In the beginning of the year 1990s, NLP started growing faster and achieved good process accuracy, especially in English Grammar. In 1990 also, an electronic text introduced, which provided a good resource for training and examining natural language programs. Other factors may include the availability of computers with fast CPUs and more memory. The major factor behind the advancement of natural language processing was the Internet.

Now, modern NLP consists of various applications, like speech recognition, machine translation, and machine text reading. When we combine all these applications then it allows the artificial intelligence to gain knowledge of the world. Let's consider the example of AMAZON ALEXA, using this robot you can ask the question to Alexa, and it will reply to you.


## Sources and Credits 

* [Evolution and Future of Natural Language Processing (NLP) By Jagreet Kaur | 04 December 2019](https://www.xenonstack.com/blog/evolution-of-nlp) 


